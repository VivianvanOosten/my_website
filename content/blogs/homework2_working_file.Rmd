---
title: "Session 4: Homework 2"
author: "Group 12: Samarth Sharma, Vivian van Oosten, Anastasia Fu, Jaelyn Shi, Andrew Robak, Shivant Maharaj"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
editor_options: 
  markdown: 
    wrap: 72
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```

```{r load-libraries, include=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
library(wbstats)
library(countrycode)
library(patchwork)
library(gganimate)
library(scales)
library(infer)
```

# Climate change and temperature anomalies

We are analysing a dataset from NASA's Goddard Institute for Space
Studies to study the effects of climate change in the Northern
Hemisphere. Glimpsing at the data, we see there are 19 variables and 143
observations, representing the period between 1880-2022:

```{r weather_data, cache=TRUE}

weather <- 
  read_csv("https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv", 
           skip = 1, 
           na = "***")
glimpse(weather)

```

For the purpose of our analysis, we have decided to select only data
pertaining to temperature deviation (delta) by month, and manipulate the
dataframe to facilitate further investigation:

```{r tidyweather}

tidyweather <- select(weather, 1:13) %>% 
  pivot_longer(!Year, names_to = 'month', values_to = 'delta')
head(tidyweather)
  

```

## Plotting Information

First, we are plotting a scatter plot to visualize the evolution of
delta (temperature deviation) over time:

```{r scatter_plot}

tidyweather <- tidyweather %>%
  mutate(date = ymd(paste(as.character(Year), month, "1")),
         month = month(date, label=TRUE),
         year = year(date))

ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point()+
  geom_smooth(color="red") +
  theme_bw() +
  labs (
    title = "Weather Anomalies"
  )

```

Adding a line of best fit to the scatterplot clearly shows that, while
deltas were close to 0 between approximately 1935-1975 and negative
before that time, temperature in the Northern Hempishere has been
quickly increasing since then. Notice that the rate of the increase has
been increasing as well (as signified by increasing deltas).

Next, we will use `facet_wrap()` to visualize the effects of increasing
temperature by month:

```{r facet_wrap, echo=FALSE}

ggplot(tidyweather, aes(x= year, y = delta))+
  geom_point()+
  geom_smooth(color="red") +
  facet_wrap(~month)+
  theme_bw() +
  labs (
    title = "Weather Anomalies"
  )

```

We can see that the effect of rising temperature in the Northern
Hemisphere is common to all months of the year.

As a means to further investigate the effects of climate change, we will
partition the data into time periods, particularly decades. To that end,
we will use `case_when()`:

```{r intervals}

comparison <- tidyweather %>% 
  filter(Year>= 1881) %>%     #remove years prior to 1881
  #create new variable 'interval', and assign values based on criteria below:
  mutate(interval = case_when(
    Year %in% c(1881:1920) ~ "1881-1920",
    Year %in% c(1921:1950) ~ "1921-1950",
    Year %in% c(1951:1980) ~ "1951-1980",
    Year %in% c(1981:2010) ~ "1981-2010",
    TRUE ~ "2011-present"
  ))

```

In order to study the effects of climate change by decade, we will
produce a density plot to investigate the distribution of monthly
temperature deviations by the time periods selected above:

```{r density_plot }

ggplot(comparison) +
  aes(x = delta, fill = interval)+
  #facet_wrap(~month)+
  geom_density(alpha = 0.2) 


```

The plot clearly shows that with the passage of time, deltas have
consistently moved to the right hand side of the plot. In other words,
temperature deviations have been increasing over time.

Lastly, we will also consider annual anomalies by grouping the monthly
data and producing a scatterplot:

```{r averaging }

#creating yearly averages
average_annual_anomaly <- tidyweather %>% 
  group_by(Year) %>%   #grouping data by Year
  
  # creating summaries for mean delta 
  # use `na.rm=TRUE` to eliminate NA (not available) values 
  summarise(yearly_mean = mean(delta, na.rm=TRUE)) 
  
average_annual_anomaly
#plotting the data
#Fit the best fit line, using LOESS method
ggplot(average_annual_anomaly) +
  aes(x = Year, y = yearly_mean)+
  geom_point()+
  geom_smooth(method = 'lm') +
  theme_bw()
```

The plot proves that annual temprature deltas have been increasing over
time.

## Confidence Interval for `delta`

We will now focus on the time period between 2011-present. We divert our
attention towards producing a confidence interval for the average annual
deltas calculated since 2011. We will use both the statistical method
and bootstrap simulation to have more confidence in the results:

```{r, calculate_CI_using_formula }

#statistical method
formula_ci <- comparison %>% 
  filter(interval == '2011-present') %>% 
  group_by(year) %>% 
  summarise(avg_annual_delta = mean(delta),
            sd_delta = sd(delta),
            count = n(),
            SE = sd(delta/sqrt(count)),
            upper_ci = avg_annual_delta + 2*SE,
            lower_ci = avg_annual_delta - 2*SE)

#bootstrap simulation  
formula_ci_2 <- comparison %>% 
  filter(interval == '2011-present') %>% 
  specify(response = delta) %>% 
  generate(type = 'bootstrap') %>% 
  calculate(stat = 'mean') %>% 
  get_confidence_interval(level = 0.95)

#print out formula_CI
formula_ci
formula_ci_2

```

Looking at the results of the analysis, we can see that the statistical
method produces wider confidence intervals for temperature deltas,
ranging from 0.13 to approximately 0.3 in width. This is probably due to
the low number of observations (12 months in each year), which prohibit
a more precise calculation. On the other hand, using bootstrap
simulation allows to get a much narrower confidence interval. However,
both methods show that temperature deltas have been positive in the time
period in question and have been consistently greater than 1 since 2015.

# Biden's Approval Margins

In this section, we will analyse the evolution of approval margins of US
President Joe Biden. Glimpsing at the dataset, we notice there are 22
variables and 4,495 observations:

```{r, cache=TRUE}
# Import approval polls data directly off fivethirtyeight website
approval_polllist <- read_csv('https://projects.fivethirtyeight.com/biden-approval-data/approval_polllist.csv') 

glimpse(approval_polllist)
```

## Create a plot

We will first calculate the net approval rate (approve - disapprove) for
each week in 2022 along with its 95% confidence interval, and then plot
the results as a line plot grouping by respondent group (Adults, Voters,
All Groups).

```{r question2}

fixed_dates <- approval_polllist %>%
  mutate(date = mdy(enddate),
         weeks = week(date),
         year = year(date),
         net_rate = approve - disapprove) %>%
  filter(year == 2022, weeks<50) %>%
  group_by(subgroup , weeks) %>%
  
  # we calculated the 95% confidence interval. 
  summarise(mean_rate = mean(net_rate,na.rm=TRUE),
            sd_rate = sd(net_rate,na.rm=TRUE),
            number = n(),
            t_critical = qt(0.975,number-1),
            lower_bound = mean_rate - t_critical*sd_rate/ sqrt(number),
            upper_bound = mean_rate + t_critical*sd_rate/ sqrt(number)) 

# we draw the graph of the net approval rate changing over weeks with its confidence interval.
  ggplot(fixed_dates, aes(x = weeks, y = mean_rate, color = subgroup)) + 
  geom_ribbon(aes(ymin = lower_bound, ymax = upper_bound), 
              fill = "orange", 
              alpha = 0.25, 
              show.legend = FALSE) +
    facet_grid(rows = vars(subgroup)) +
    geom_line(aes(x = weeks, y = mean_rate, 
                  color = subgroup),
              show.legend = FALSE) +
    labs(title = "Biden's net approval ratings in 2022",
         subtitle = "Weekly Data, Approve - Disapprove, %",
         caption = "Source: https://projects.fivethirtyeight.com/biden-approval-data/",
         x = NULL, 
         y = NULL) +
    theme_minimal()


```

We can see President Biden's net approval relative has remained negative
since the first week of 2022 among all poll respondents, meaning more
people disapprove than approve of the President. Furthermore, we notice
a sharp decline in the net approval rate beginning in week 23. Since
that time, the approval rate seems to have returned to pre-drop levels,
potentially due to the POTUS's recent delivery on several campaign
promises, primarily passing the Inflation Reduction Act.

# Challenge 1: Excess rentals in TfL bike sharing

This section focuses on analysing data on rentals in Tfl bike sharing.

```{r, get_tfl_data, cache=TRUE}
url <- "https://data.london.gov.uk/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xlsx"

# Download TFL data to temporary file
httr::GET(url, write_disk(bike.temp <- tempfile(fileext = ".xlsx")))

# Use read_excel to read it as dataframe
bike0 <- read_excel(bike.temp,
                   sheet = "Data",
                   range = cell_cols("A:B"))

# change dates to get year, month, and week
bike <- bike0 %>% 
  clean_names() %>% 
  rename (bikes_hired = number_of_bicycle_hires) %>% 
  mutate (year = as.integer(year(day)),
          month = lubridate::month(day, label = TRUE),
          week = isoweek(day))

glimpse(bike)

```

First, we calculate the monthly change in Tfl bike rentals, calculated
as the difference between the actual monthly average and the historical
monthly average calculated between 2016-2019. We plot the data faceting
by year, using `geom_ribbon()` to visualize the positive/negative
deltas.

```{r challenge1.1}
#calculating expected number of rentals
compare <- bike %>%
  filter(year %in% c(2016:2019)) %>% 
  group_by(month) %>% 
  summarise(compare_avg = mean(bikes_hired))

#calculating monthly averages
avg <- bike %>% 
  filter(year %in% 2017:2022) %>% 
  group_by(year, month) %>% 
  summarise(actual_avg = mean(bikes_hired))
#joining datasets
left_join(avg, compare, by = 'month') %>%
  #calculating differences
  mutate(difference = actual_avg - compare_avg, 
         pos_diff = ifelse(difference > 0, actual_avg, 0),
         neg_diff = ifelse(difference < 0, compare_avg, 0)) %>%
  #plotting
  ggplot(aes(x = month)) +
  geom_line(aes(y = compare_avg, group = 1), color = "blue", lwd = 1.5) +
  geom_line(aes(y = actual_avg, group = 1)) +
  geom_ribbon(aes(ymin = compare_avg, ymax = pmax(0, difference) + compare_avg, fill = "red", alpha = 0.5, group = 1)) +
  geom_ribbon(aes(ymin = pmin(0, difference) + compare_avg, ymax = compare_avg, fill = "green", alpha = 0.5, group = 1)) +
  facet_wrap(vars(year)) +
  labs(title = "Monthly changes in Tfl bike rentals",
       subtitle = "Change from monthly average shown in blue and calculated between 2016-2019",
       caption = "Source: Tfl, London Data Store",
       x = NULL,
       y = "Bike rentals") +
  theme(legend.position = "none",
        axis.text.x = element_text(size = 6))
```

We can see that Tfl bike rentals have been lower than in the 2016-2019
at the beginning of the pandemic, but quickly recovered and exceeded
historical data. Interestingly, there has been a significant uptake
starting in the second half of 2021, possibly due to changing
preferences regarding means of transport, with public transport losing
users to Tfl bikes.

Next, we plot a similar graph to visualize weekly changes in Tfl bike
rentals between actual data and the 2016-2019 average.

```{r challenge1.2}
#calculating expected number of rentals
compare <- bike %>%
  filter(year %in% c(2016:2019)) %>% 
  group_by(week) %>% 
  summarise(compare_avg = mean(bikes_hired))

#calculating weekly averages
avg <- bike %>% 
  filter(year %in% 2017:2022) %>% 
  group_by(year, week) %>% 
  summarise(actual_avg = mean(bikes_hired))

#deleting aberrant observations (average for future weeks in 2022)
avg <- avg[-298,]

#joining dataframes
left_join(avg, compare, by = 'week') %>%
  #calculating differences
  mutate(diff = (actual_avg - compare_avg)/compare_avg,
         pos_diff = ifelse(diff > 0, diff, 0),
         neg_diff = ifelse(diff < 0, diff, 0)) %>%
  #plotting
  ggplot(aes(x = week, y = diff)) +
  scale_x_discrete(limits = c(13, 26, 39, 53)) +
  scale_y_continuous(labels = percent) +
  geom_rect(aes(xmin = 13, xmax = 26, ymin = -Inf, ymax = Inf), alpha = 0.3, fill = "grey90") +
  geom_rect(aes(xmin = 39, xmax = 53, ymin = -Inf, ymax = Inf), alpha = 0.3, fill = "grey90") +
  geom_line(aes(y = diff, group = 1), color = 'black', lwd = 0.8) +
  geom_ribbon(aes(ymin = 0, ymax = pmax(0, pos_diff)), fill = 'green', alpha = 0.3) +
  geom_ribbon(aes(ymin = pmin(0, neg_diff), ymax = 0), fill = 'red', alpha = 0.3) +
  geom_rug(aes(colour = diff), 
           sides = 'b', 
           length = unit(0.02, "npc"), 
           size = 1, 
           show.legend = FALSE) +
  binned_scale(aesthetics = "colour",
               scale_name = "stepsn",
               palette = function(x) c("red", "green"),
               breaks = c(0, 100)) +
  facet_wrap(vars(year)) +
  theme_minimal() +
  labs(title = "Weekly changes in Tfl bike rentals",
       subtitle = "% change from weekly averages calculated between 2016-2019",
       caption = "Source: Tfl, London Data Store",
       x = "Week",
       y = NULL)
```

Again, one can easily notice the drops at the beginning of 2020 (start
of the pandemic) and in winter of 2021 (COVID wave), as well as the
sizable increase in Tfl rentals since the second half of 2021.

It should be noted that the mean has been used to calculate the expected
number of bike rentals for each month/week since the data follows a
normal distribution, as seen in the histogram below. Otherwise, it would
have been optimal to use the median instead, as it is a more robust
measure of central tendency.

```{r outliers}
hist(bike$bikes_hired)
```

# Challenge 2: Share of renewable energy production in the world

This last section focuses on analysing the share of renewable energy
production around the world. We will be using datasets from the National
Bureau of Economic Research (NBER) and the World Bank.

The following is a description of the variables from the NBER dataset:

| **variable** | **class** | **description**                |
|--------------|-----------|--------------------------------|
| variable     | character | Variable name                  |
| label        | character | Label for variable             |
| iso3c        | character | Country code                   |
| year         | double    | Year                           |
| group        | character | Group (consumption/production) |
| category     | character | Category                       |
| value        | double    | Value (related to label)       |

```{r,load_technology_data, cache=TRUE}

technology <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-07-19/technology.csv')

#get all technologies
labels <- technology %>% 
  distinct(variable, label)

# Get country names using 'countrycode' package
technology <- technology %>% 
  filter(iso3c != "XCD") %>% 
  mutate(iso3c = recode(iso3c, "ROM" = "ROU"),
         country = countrycode(iso3c, origin = "iso3c", destination = "country.name"),
         country = case_when(
           iso3c == "ANT" ~ "Netherlands Antilles",
           iso3c == "CSK" ~ "Czechoslovakia",
           iso3c == "XKX" ~ "Kosovo",
           TRUE           ~ country))

#make smaller dataframe on energy
energy <- technology %>% 
  filter(category == "Energy")

# download CO2 per capita from World Bank using {wbstats} package
# https://data.worldbank.org/indicator/EN.ATM.CO2E.PC
co2_percap <- wb_data(country = "countries_only", 
                      indicator = "EN.ATM.CO2E.PC", 
                      start_date = 1970, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated))

# get a list of countries and their characteristics
# we just want to get the region a country is in and its income level
countries <-  wb_cachelist$countries %>% 
  select(iso3c,region,income_level)

```

First, produce a graph with the countries with the highest and lowest %
contribution of renewables in energy production.

```{r challenge2.1}

#manipulating the dataset
data <- energy %>% 
  filter(year == 2019) %>% 
  select(country, variable, value) %>% 
  pivot_wider(names_from = "variable", values_from = "value") %>% 
  arrange(country) %>% 
  mutate(renew_share = (elec_solar + elec_hydro + elec_wind + elec_renew_other)/elecprod,
         renew_rounded = round(renew_share, digits = 4)) %>% 
  drop_na(renew_rounded) %>% 
  filter(renew_rounded > 0) %>% 
  arrange(desc(renew_rounded)) %>% 
  select(country, renew_rounded)

#selecting top20 observations
max <- data %>%
  slice_max(n = 20, order_by = renew_rounded)

#plotting  
max_plot <- ggplot(max, aes(x = renew_rounded, y = reorder(country, renew_rounded))) +
            geom_bar(stat = 'identity') +
            scale_x_continuous(labels = percent) +
            labs(x = NULL, y = NULL)

#selecting min20 observations
min <- data %>% 
  slice_min(n = 20, order_by = renew_rounded)

#plotting
min_plot <- ggplot(min, aes(x = renew_rounded, y = reorder(country, renew_rounded))) +
            geom_bar(stat = 'identity') +
            scale_x_continuous(labels = percent) +
            labs(x = NULL, y = NULL)

library(patchwork)
full_plot <- max_plot + min_plot 

full_plot + plot_annotation(
  title = "Highest and lowest % in energy production",
  subtitle = "2019 data",
  caption = "NBER CHAT Database")

```

We can see that countries with the highest share of renewables in their
energy mix are not necessarily the richest nations, and 8 states in that
group get 100% of their energy from renewable sources. Similarly, the
ones with the lowest % of renewables used to produce energy include some
of the wealthiest states in the world (Kuwait, Qatar), which is likely
due to their access to vast amounts of oil.

Second, we can produce an animation to explore the relationship between
CO2 per capita emissions and the deployment of renewables over time,
faceted by income group.

```{r challenge2.2}

#manipulating the data to facilitate time-series analysis
renewables <- energy %>% 
  select(iso3c, year, country, variable, value) %>% 
  pivot_wider(names_from = "variable", values_from = "value") %>% 
  arrange(country) %>% 
  mutate(renew_share = (elec_solar + elec_hydro + elec_wind + elec_renew_other)/elecprod,
         renew_rounded = round(renew_share, digits = 4)) %>% 
  drop_na(renew_rounded) %>% 
  filter(renew_rounded > 0) %>% 
  select(iso3c, year, country, renew_rounded)

#deleting aberrant observations (share of renewables > 100%)
renewables <- renewables[-c(1:3), ]

#joining datasets to create a single dataframe
joined_first <- left_join(renewables, co2_percap, by=c("iso3c" = "iso3c", "year" = "date"))
full_data <- left_join(joined_first, countries, by = "iso3c")

#filtering the dataset
for_plots <- full_data %>% 
  mutate(year = as.integer(year)) %>% 
  select(year, country.x, renew_rounded, value, income_level) %>%
  drop_na(income_level, value) %>% 
  filter(year %in% 1990:2020)

#plotting
plot <- for_plots %>% 
  ggplot(aes(x = renew_rounded, y = value, color = income_level)) +
  geom_point() +
  ylim(0, 50) +
  facet_wrap(vars(income_level)) +
  labs(title = 'Year: {frame_time}', 
       x = '% renewables', 
       y = 'CO2 per cap') +
  transition_time(year) +
  ease_aes('linear') +
  theme(legend.position = "none") +
  scale_x_continuous(labels = percent)

for_plots
plot
```

The animation shows that as the % of energy generated by renewables goes
up, CO2 per capita emissions do not seem to go down. This would be
signified by dots moving diagonally from the upper-left to the
lower-right corner of the plot. However, no such movement is detected.
In fact, we can observe that quite a few countries move horizontally to
the right, which means that their CO2 per capita emissions stay constant
as the share of renewables in their energy mix increases.

# Details

-   Who did you collaborate with: Samarth Sharma, Vivian van Oosten,
    Anastasia Fu, Jaelyn Shi, Andrew Robak, Shivant Maharaj
-   Approximately how much time did you spend on this problem set: 10h
-   What, if anything, gave you the most trouble: shadowing the two
    quarters in challenge 1
